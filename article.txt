
@article{wang_integrating_2022,
	title = {Integrating {Expert} {Knowledge} {With} {Domain} {Adaptation} for {Unsupervised} {Fault} {Diagnosis}},
	volume = {71},
	issn = {1557-9662},
	doi = {10.1109/TIM.2021.3127654},
	abstract = {Data-driven fault diagnosis methods often require abundant labeled examples for each fault type. On the contrary, real-world data is often unlabeled and consists of mostly healthy observations and only few samples of faulty conditions. The lack of labels and fault samples imposes a significant challenge for existing data-driven fault diagnosis methods. In this article, we aim to overcome this limitation by integrating expert knowledge with domain adaptation (DA) in a synthetic-to-real framework for unsupervised fault diagnosis. Motivated by the fact that domain experts often have a relatively good understanding on how different fault types affect healthy signals, in the first step of the proposed framework, a synthetic fault dataset is generated by augmenting real vibration samples of healthy bearings. This synthetic dataset integrates expert knowledge and encodes class information about the faults types. However, models trained solely based on the synthetic data often do not perform well because of the distinct distribution difference between the synthetically generated and real faults. To overcome this domain gap between the synthetic and real data, in the second step of the proposed framework, an imbalance-robust DA approach is proposed to adapt the model from synthetic faults (source) to the unlabeled real faults (target) which suffer from severe class imbalance. The framework is evaluated on two unsupervised fault diagnosis cases for bearings, the CWRU laboratory dataset and a real-world wind-turbine dataset. Experimental results demonstrate that the generated faults are effective for encoding fault type information and the DA is robust against the different levels of class imbalance between faults.},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Wang, Qin and Taal, Cees and Fink, Olga},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Instrumentation and Measurement},
	keywords = {domain adaptation, fault diagnosis, synthetic, notion},
	pages = {1--12},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\bahi_\\Zotero\\storage\\Y8UWZ4SR\\9612159.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\C6WQ2XAQ\\Wang et al. - 2022 - Integrating Expert Knowledge With Domain Adaptatio.pdf:application/pdf},
}

@inproceedings{wang_domain_2021,
	address = {Montreal, QC, Canada},
	title = {Domain {Adaptive} {Semantic} {Segmentation} with {Self}-{Supervised} {Depth} {Estimation}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710353/},
	doi = {10.1109/ICCV48922.2021.00840},
	abstract = {Domain adaptation for semantic segmentation aims to improve the model performance in the presence of a distribution shift between source and target domain. Leveraging the supervision from auxiliary tasks (such as depth estimation) has the potential to heal this shift because many visual tasks are closely related to each other. However, such a supervision is not always available. In this work, we leverage the guidance from self-supervised depth estimation, which is available on both domains, to bridge the domain gap. On the one hand, we propose to explicitly learn the task feature correlation to strengthen the target semantic predictions with the help of target depth estimation. On the other hand, we use the depth prediction discrepancy from source and target depth decoders to approximate the pixel-wise adaptation difﬁculty. The adaptation difﬁculty, inferred from depth, is then used to reﬁne the target semantic segmentation pseudo-labels. The proposed method can be easily implemented into existing segmentation frameworks. We demonstrate the effectiveness of our approach on the benchmark tasks SYNTHIA-to-Cityscapes and GTA-to-Cityscapes, on which we achieve the new stateof-the-art performance of 55.0\% and 56.6\%, respectively. Our code is available at https://qin.ee/corda.},
	language = {en},
	urldate = {2022-09-05},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wang, Qin and Dai, Dengxin and Hoyer, Lukas and Van Gool, Luc and Fink, Olga},
	month = oct,
	year = {2021},
	keywords = {domain adaptation, semantic segmentation, notion},
	pages = {8495--8505},
	file = {Wang et al. - 2021 - Domain Adaptive Semantic Segmentation with Self-Su.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\IARNU3GS\\Wang et al. - 2021 - Domain Adaptive Semantic Segmentation with Self-Su.pdf:application/pdf},
}

@inproceedings{wang_domain_2019,
	title = {Domain {Adaptive} {Transfer} {Learning} for {Fault} {Diagnosis}},
	doi = {10.1109/PHM-Paris.2019.00054},
	abstract = {Thanks to digitization of industrial assets in fleets, the ambitious goal of transferring fault diagnosis models from one machine to the other has raised great interest. Solving these domain adaptive transfer learning tasks has the potential to save large efforts on manually labeling data and modifying models for new machines in the same fleet. Although data-driven methods have shown great potential in fault diagnosis applications, their ability to generalize on new machines and new working conditions are limited because of their tendency to overfit to the training set in reality. One promising solution to this problem is to use domain adaptation techniques. It aims to improve model performance on the target new machine. Inspired by its successful implementation in computer vision, we introduced Domain-Adversarial Neural Networks (DANN) to our context, along with two other popular methods existing in previous fault diagnosis research. We then carefully justify the applicability of these methods in realistic fault diagnosis settings, and offer a unified experimental protocol for a fair comparison between domain adaptation methods for fault diagnosis problems.},
	booktitle = {2019 {Prognostics} and {System} {Health} {Management} {Conference} ({PHM}-{Paris})},
	author = {Wang, Qin and Michau, Gabriel and Fink, Olga},
	month = may,
	year = {2019},
	note = {ISSN: 2166-5656},
	keywords = {domain adaptation, fault diagnosis, notion},
	pages = {279--285},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\bahi_\\Zotero\\storage\\SWP7NLYW\\8756463.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\I6MDC4LH\\Wang et al. - 2019 - Domain Adaptive Transfer Learning for Fault Diagno.pdf:application/pdf},
}

@inproceedings{wang_continual_2022,
	title = {Continual {Test}-{Time} {Domain} {Adaptation}},
	abstract = {Test-time domain adaptation aims to adapt a source pretrained model to a target domain without using any source data. Existing works mainly consider the case where the target domain is static. However, real-world machine perception systems are running in non-stationary and continually changing environments where the target domain distribution can change over time. Existing methods, which are mostly based on self-training and entropy regularization, can suffer from these non-stationary environments. Due to the distribution shift over time in the target domain, pseudo-labels become unreliable. The noisy pseudolabels can further lead to error accumulation and catastrophic forgetting. To tackle these issues, we propose a continual test-time adaptation approach (CoTTA) which comprises two parts. Firstly, we propose to reduce the error accumulation by using weight-averaged and augmentationaveraged predictions which are often more accurate. On the other hand, to avoid catastrophic forgetting, we propose to stochastically restore a small part of the neurons to the source pre-trained weights during each iteration to help preserve source knowledge in the long-term. The proposed method enables the long-term adaptation for all parameters in the network. CoTTA is easy to implement and can be readily incorporated in off-the-shelf pre-trained models. We demonstrate the effectiveness of our approach on four classification tasks and a segmentation task for continual testtime adaptation, on which we outperform existing methods. Our code is available at https://qin.ee/cotta.},
	language = {en},
	booktitle = {2022 {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Qin and Fink, Olga and Gool, Luc Van and Dai, Dengxin},
	month = mar,
	year = {2022},
	keywords = {domain adaptation, test-time, notion},
	pages = {11},
	file = {Wang et al. - Continual Test-Time Domain Adaptation.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\FSAKJ5US\\Wang et al. - Continual Test-Time Domain Adaptation.pdf:application/pdf},
}

@article{li_cross-domain_2019,
	title = {Cross-{Domain} {Fault} {Diagnosis} of {Rolling} {Element} {Bearings} {Using} {Deep} {Generative} {Neural} {Networks}},
	volume = {66},
	issn = {1557-9948},
	doi = {10.1109/TIE.2018.2868023},
	abstract = {Despite the recent advances on intelligent fault diagnosis of rolling element bearings, existing research works mostly assume training and testing data are drawn from the same distribution. However, due to variation of operating condition, domain shift phenomenon generally exists, which results in significant diagnosis performance deterioration. To address cross-domain problems, latest research works preferably apply domain adaptation techniques on marginal data distributions. However, it is usually assumed that sufficient testing data are available for training, that is not in accordance with most transfer tasks in real industries where only data in machine healthy condition can be collected in advance. This paper proposes a novel cross-domain fault diagnosis method based on deep generative neural networks. By artificially generating fake samples for domain adaptation, the proposed method is able to provide reliable cross-domain diagnosis results when testing data in machine fault conditions are not available for training. The experimental results suggest that the proposed method offers a promising approach for industrial applications.},
	number = {7},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Li, Xiang and Zhang, Wei and Ding, Qian},
	month = jul,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Industrial Electronics},
	keywords = {domain adaptation, fault diagnosis, generative model, bearings, notion},
	pages = {5525--5534},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\bahi_\\Zotero\\storage\\B2W5WXAR\\8456850.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\GXSIBRHU\\Li et al. - 2019 - Cross-Domain Fault Diagnosis of Rolling Element Be.pdf:application/pdf},
}

@article{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/15-239.html},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.

The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages.

We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
	number = {59},
	urldate = {2022-09-05},
	journal = {Journal of Machine Learning Research},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and March, Mario and Lempitsky, Victor},
	year = {2016},
	keywords = {domain adaptation, notion},
	pages = {1--35},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\Q27RGYCB\\Ganin et al. - 2016 - Domain-Adversarial Training of Neural Networks.pdf:application/pdf},
}

@inproceedings{ganin_unsupervised_2015,
	address = {Lille, France},
	title = {Unsupervised {Domain} {Adaptation} by {Backpropagation}},
	volume = {37},
	abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled targetdomain data is necessary).},
	language = {en},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	author = {Ganin, Yaroslav and Lempitsky, Victor},
	year = {2015},
	keywords = {domain adaptation, notion},
	pages = {10},
	file = {Ganin and Lempitsky - Unsupervised Domain Adaptation by Backpropagation.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\R5YF7QFH\\Ganin and Lempitsky - Unsupervised Domain Adaptation by Backpropagation.pdf:application/pdf;suppmat.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\SNW4A8HC\\suppmat.pdf:application/pdf},
}

@article{ben-david_theory_2010,
	title = {A theory of learning from different domains},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
	year = {2010},
	keywords = {domain adaptation, notion},
	pages = {25},
	file = {Ben-David et al. - 2010 - A theory of learning from different domains.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\59BVRVQW\\Ben-David et al. - 2010 - A theory of learning from different domains.pdf:application/pdf},
}

@article{borgwardt_integrating_2006,
	title = {Integrating structured biological data by {Kernel} {Maximum} {Mean} {Discrepancy}},
	volume = {22},
	issn = {1367-4803, 1460-2059},
	url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btl242},
	doi = {10.1093/bioinformatics/btl242},
	abstract = {Motivation: Many problems in data integration in bioinformatics can be posed as one common question: Are two sets of observations generated by the same distribution? We propose a kernel-based statistical test for this problem, based on the fact that two distributions are different if and only if there exists at least one function having different expectation on the two distributions. Consequently we use the maximum discrepancy between function means as the basis of a test statistic.},
	language = {en},
	number = {14},
	urldate = {2022-09-08},
	journal = {Bioinformatics},
	author = {Borgwardt, K. M. and Gretton, A. and Rasch, M. J. and Kriegel, H.-P. and Scholkopf, B. and Smola, A. J.},
	month = jul,
	year = {2006},
	keywords = {domain adaptation, notion},
	pages = {e49--e57},
	file = {Borgwardt et al. - 2006 - Integrating structured biological data by Kernel M.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\BM3SUWP8\\Borgwardt et al. - 2006 - Integrating structured biological data by Kernel M.pdf:application/pdf},
}

@inproceedings{gong_connecting_2013,
	address = {Atlanta},
	title = {Connecting the {Dots} with {Landmarks}: {Discriminatively} {Learning} {Domain}-{Invariant} {Features} for {Unsupervised} {Domain} {Adaptation}},
	abstract = {Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classiﬁers trained on the source domain need to be adapted to a diﬀerent target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-ofthe-art signiﬁcantly.},
	language = {en},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	author = {Gong, Boqing and Grauman, Kristen and Sha, Fei},
	year = {2013},
	keywords = {domain adaptation, notion},
	pages = {9},
	file = {Gong et al. - Connecting the Dots with Landmarks Discriminative.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\JJH4ENPT\\Gong et al. - Connecting the Dots with Landmarks Discriminative.pdf:application/pdf},
}

@inproceedings{fernando_unsupervised_2013,
	title = {Unsupervised {Visual} {Domain} {Adaptation} {Using} {Subspace} {Alignment}},
	doi = {10.1109/ICCV.2013.368},
	abstract = {In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces described by eigenvectors. In this context, our method seeks a domain adaptation solution by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We use a theoretical result to tune the unique hyper parameter corresponding to the size of the subspaces. We run our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Fernando, Basura and Habrard, Amaury and Sebban, Marc and Tuytelaars, Tinne},
	month = dec,
	year = {2013},
	note = {ISSN: 2380-7504},
	keywords = {domain adaptation, subspace alignment, notion},
	pages = {2960--2967},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\bahi_\\Zotero\\storage\\QFUL3HH6\\6751479.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\AZJBW2LJ\\Fernando et al. - 2013 - Unsupervised Visual Domain Adaptation Using Subspa.pdf:application/pdf},
}

@inproceedings{cortes_domain_2011,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Domain {Adaptation} in {Regression}},
	isbn = {978-3-642-24412-4},
	doi = {10.1007/978-3-642-24412-4_25},
	abstract = {This paper presents a series of new results for domain adaptation in the regression setting. We prove that the discrepancy is a distance for the squared loss when the hypothesis set is the reproducing kernel Hilbert space induced by a universal kernel such as the Gaussian kernel. We give new pointwise loss guarantees based on the discrepancy of the empirical source and target distributions for the general class of kernel-based regularization algorithms. These bounds have a simpler form than previous results and hold for a broader class of convex loss functions not necessarily differentiable, including Lqlosses and the hinge loss. We extend the discrepancy minimization adaptation algorithm to the more significant case where kernels are used and show that the problem can be cast as an SDP similar to the one in the feature space. We also show that techniques from smooth optimization can be used to derive an efficient algorithm for solving such SDPs even for very high-dimensional feature spaces. We have implemented this algorithm and report the results of experiments demonstrating its benefits for adaptation and show that, unlike previous algorithms, it can scale to large data sets of tens of thousands or more points.},
	language = {en},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer},
	author = {Cortes, Corinna and Mohri, Mehryar},
	editor = {Kivinen, Jyrki and Szepesvári, Csaba and Ukkonen, Esko and Zeugmann, Thomas},
	year = {2011},
	keywords = {domain adaptation, notion},
	pages = {308--323},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\NWFA43UD\\Cortes and Mohri - 2011 - Domain Adaptation in Regression.pdf:application/pdf},
}

@misc{tzeng_deep_2014,
	title = {Deep {Domain} {Confusion}: {Maximizing} for {Domain} {Invariance}},
	shorttitle = {Deep {Domain} {Confusion}},
	url = {http://arxiv.org/abs/1412.3474},
	doi = {10.48550/arXiv.1412.3474},
	abstract = {Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task.},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},
	month = dec,
	year = {2014},
	note = {arXiv:1412.3474 [cs]
version: 1},
	keywords = {domain adaptation, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\C454BWZ4\\Tzeng et al. - 2014 - Deep Domain Confusion Maximizing for Domain Invar.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\54WSGQJQ\\1412.html:text/html},
}

@inproceedings{chopra_dlid_2013,
	title = {{DLID}: {Deep} learning for domain adaptation by interpolating between domains},
	shorttitle = {{DLID}},
	abstract = {In many real world applications of machine learning, the distribution of the training data (on which the machine learning model is trained) is different from the distribution of the test data (where the learnt model is actu-ally deployed). This is known as the problem of Domain Adaptation. We propose a novel deep learning model for domain adaptation which attempts to learn a predictively useful representation of the data by taking into ac-count information from the distribution shift between the training and test data. Our key proposal is to successively learn multiple in-termediate representations along an “inter-polating path ” between the train and test do-mains. Our experiments on a standard object recognition dataset show a significant perfor-mance improvement over the state-of-the-art. 1. Problem Motivation and Context Oftentimes in machine learning applications, we have to learn a model to accomplish a specific task using training data drawn from one distribution (the source domain), and deploy the learnt model on test data drawn from a different distribution (the target do-main). For instance, consider the task of creating a mobile phone application for “image search for prod-ucts”; where the goal is to look up product specifi-cations and comparative shopping options from the internet, given a picture of the product taken with a user’s mobile phone. In this case, the underlying ob-ject recognizer will typically be trained on a labeled corpus of images (perhaps scraped from the internet), and tested on the images taken using the user’s phone camera. The challenge here is that the distribution of training and test images is not the same. A naively},
	booktitle = {in {ICML} {Workshop} on {Challenges} in {Representation} {Learning}},
	author = {Chopra, Sumit and Balakrishnan, Suhrid and Gopalan, Raghuraman},
	year = {2013},
	keywords = {domain adaptation, notion},
	file = {Citeseer - Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\4XQEY243\\Chopra et al. - 2013 - DLID Deep learning for domain adaptation by inter.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\RXGTYKVP\\summary.html:text/html},
}

@inproceedings{ben-david_analysis_2007,
	title = {Analysis of {Representations} for {Domain} {Adaptation}},
	abstract = {Discriminative learning methods for classiﬁcation perform well when training and test data are drawn from the same distribution. In many situations, though, we have labeled training data for a source domain, and we wish to learn a classiﬁer which performs well on a target domain with a different distribution. Under what conditions can we adapt a classiﬁer trained on the source domain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition theoretically with a generalization bound for domain adaption. Our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justiﬁcation for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the difference between the source and target domains, while at the same time maximizing the margin of the training set.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NIPS})},
	author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
	year = {2007},
	keywords = {domain adaptation, notion},
	pages = {8},
	file = {Ben-David et al. - Analysis of Representations for Domain Adaptation.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\YSTZGDHQ\\Ben-David et al. - Analysis of Representations for Domain Adaptation.pdf:application/pdf},
}

@inproceedings{long_learning_2015,
	title = {Learning {Transferable} {Features} with {Deep} {Adaptation} {Networks}},
	url = {https://proceedings.mlr.press/v37/long15.html},
	abstract = {Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.},
	language = {en},
	urldate = {2022-09-08},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	keywords = {domain adaptation, notion},
	pages = {97--105},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\D6NYWXNJ\\Long et al. - 2015 - Learning Transferable Features with Deep Adaptatio.pdf:application/pdf},
}

@article{pan_domain_2011,
	title = {Domain {Adaptation} via {Transfer} {Component} {Analysis}},
	volume = {22},
	issn = {1941-0093},
	doi = {10.1109/TNN.2010.2091281},
	abstract = {Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Pan, Sinno Jialin and Tsang, Ivor W. and Kwok, James T. and Yang, Qiang},
	month = feb,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {domain adaptation, transfer learning, notion},
	pages = {199--210},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\Z4QTQYJP\\Pan et al. - 2011 - Domain Adaptation via Transfer Component Analysis.pdf:application/pdf},
}

@misc{li_revisiting_2016,
	title = {Revisiting {Batch} {Normalization} {For} {Practical} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1603.04779},
	doi = {10.48550/arXiv.1603.04779},
	abstract = {Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study (Tommasi et al. 2015) shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.},
	urldate = {2022-09-08},
	publisher = {arXiv},
	author = {Li, Yanghao and Wang, Naiyan and Shi, Jianping and Liu, Jiaying and Hou, Xiaodi},
	month = nov,
	year = {2016},
	note = {arXiv:1603.04779 [cs]},
	keywords = {domain adaptation, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\5PGUHLVY\\Li et al. - 2016 - Revisiting Batch Normalization For Practical Domai.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\U782HV95\\1603.html:text/html},
}

@inproceedings{saito_maximum_2018,
	address = {Salt Lake City, UT, USA},
	title = {Maximum {Classifier} {Discrepancy} for {Unsupervised} {Domain} {Adaptation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578490/},
	doi = {10.1109/CVPR.2018.00392},
	abstract = {In this work, we present a method for unsupervised domain adaptation. Many adversarial learning methods train domain classiﬁer networks to distinguish the features as either a source or target and train a feature generator network to mimic the discriminator. Two problems exist with these methods. First, the domain classiﬁer only tries to distinguish the features as a source or target and thus does not consider task-speciﬁc decision boundaries between classes. Therefore, a trained generator can generate ambiguous features near class boundaries. Second, these methods aim to completely match the feature distributions between different domains, which is difﬁcult because of each domain’s characteristics.},
	language = {en},
	urldate = {2022-09-08},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Saito, Kuniaki and Watanabe, Kohei and Ushiku, Yoshitaka and Harada, Tatsuya},
	month = jun,
	year = {2018},
	keywords = {domain adaptation, notion},
	pages = {3723--3732},
	file = {Saito et al. - 2018 - Maximum Classifier Discrepancy for Unsupervised Do.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\5RTF8UK8\\Saito et al. - 2018 - Maximum Classifier Discrepancy for Unsupervised Do.pdf:application/pdf},
}

@article{barreto_unsupervised_2022,
	title = {Unsupervised {Domain} {Adaptation} using {Maximum} {Mean} {Covariance} {Discrepancy} and {Variational} {Autoencoder}},
	volume = {13},
	abstract = {Face Recognition has progressed tremendously from its initial use of holistic learning models to using handcrafted, shallow, and deep learning models. DeepFace, a ninelayer Deep Convolutional Neural Network (DCNN), reached nearhuman performance on unconstrained face recognition for the Labeled Faces in the Wild (LFW) dataset. These models performed very well on the benchmark datasets, but their performance sometimes deteriorated for real-world applications. The problem arose when there was a domain shift due to different distribution spaces of the training and testing models. Few researchers looked at Unsupervised Domain Adaptation (UDA) to find the domaininvariant feature spaces. They tried to minimize the domain discrepancy using a static loss of maximum mean discrepancy (MMD). From MMD, the researchers delved into the higher-order statistics of maximum covariance discrepancy (MCD). MMD and MCD were combined to get maximum mean and covariance discrepancy (MMCD), which captured more information than MMD alone. We use a Variational Autoencoder (VAE) with joint mean and covariance discrepancy to offer a solution for domain adaptation. The proposed MMCD-VAE model uses VAE to measure the discrepancy in the spread of variance around the mean value and uses MMCD to measure the directional discrepancy in the variance. Analysis was done using the TinyFace benchmark dataset and the Bollywood Celebrities dataset. Three objective image quality parameters, namely SSIM, pieAPP, and SIFT feature matching, demonstrate the superiority of MMCDVAE over the conventional KL-VAE model. MMCD-VAE shows an 18 \% improvement in SSIM and a remarkable improvement in the perceptual quality of the image over the conventional KLVAE model.},
	language = {en},
	number = {6},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Barreto, Fabian},
	year = {2022},
	keywords = {domain adaptation, notion},
	pages = {10},
	file = {Barreto - 2022 - Unsupervised Domain Adaptation using Maximum Mean .pdf:C\:\\Users\\bahi_\\Zotero\\storage\\69488LXJ\\Barreto - 2022 - Unsupervised Domain Adaptation using Maximum Mean .pdf:application/pdf},
}

@book{redko_advances_2019,
	title = {Advances in {Domain} {Adaptation} {Theory}},
	author = {Redko, Ievgen and Habrard, Amaury and Morvant, Emilie and Sebban, Marc and Bennani, Younès},
	year = {2019},
	keywords = {domain adaptation, notion},
	file = {1---State-of-the-Art-of-Statistical-Learni_2019_Advances-in-Domain-Adaptatio.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\VTV5V3GM\\1---State-of-the-Art-of-Statistical-Learni_2019_Advances-in-Domain-Adaptatio.pdf:application/pdf;2---Domain-Adaptation-Problem_2019_Advances-in-Domain-Adaptation-Theory.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\HDZ79D8K\\2---Domain-Adaptation-Problem_2019_Advances-in-Domain-Adaptation-Theory.pdf:application/pdf;3---Seminal-Divergence-based-Generalizati_2019_Advances-in-Domain-Adaptation.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\N2ZISXH7\\3---Seminal-Divergence-based-Generalizati_2019_Advances-in-Domain-Adaptation.pdf:application/pdf;4---Impossibility-Theorems-for-Domain-Ad_2019_Advances-in-Domain-Adaptation-.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\25FYDLKC\\4---Impossibility-Theorems-for-Domain-Ad_2019_Advances-in-Domain-Adaptation-.pdf:application/pdf;5---Generalization-Bounds-with-Integral-Prob_2019_Advances-in-Domain-Adaptat.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\SQ36FW3H\\5---Generalization-Bounds-with-Integral-Prob_2019_Advances-in-Domain-Adaptat.pdf:application/pdf;6---PAC-Bayesian-Theory-for-Domain-Adap_2019_Advances-in-Domain-Adaptation-T.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\YDIHCWFV\\6---PAC-Bayesian-Theory-for-Domain-Adap_2019_Advances-in-Domain-Adaptation-T.pdf:application/pdf;7---Domain-Adaptation-Theory-Based-on-Algori_2019_Advances-in-Domain-Adaptat.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\HFMECEJR\\7---Domain-Adaptation-Theory-Based-on-Algori_2019_Advances-in-Domain-Adaptat.pdf:application/pdf;8---Iterative-Domain-Adaptation-Metho_2019_Advances-in-Domain-Adaptation-The.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\R34BL9J6\\8---Iterative-Domain-Adaptation-Metho_2019_Advances-in-Domain-Adaptation-The.pdf:application/pdf;Abstract_2019_Advances-in-Domain-Adaptation-Theory.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\KA73NQKZ\\Abstract_2019_Advances-in-Domain-Adaptation-Theory.pdf:application/pdf;Appendix-1---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\KDCFY77K\\Appendix-1---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:application/pdf;Appendix-2---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\X44FH3IJ\\Appendix-2---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:application/pdf;Appendix-3---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\LWBD6I63\\Appendix-3---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:application/pdf;Appendix-4---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\W5XS2A9L\\Appendix-4---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:application/pdf;Appendix-5---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\542ADQR4\\Appendix-5---Proofs-of-the-Main-Results-of_2019_Advances-in-Domain-Adaptatio.pdf:application/pdf;Conclusions-and-Discussions_2019_Advances-in-Domain-Adaptation-Theory.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\DKX7GNJT\\Conclusions-and-Discussions_2019_Advances-in-Domain-Adaptation-Theory.pdf:application/pdf;Copyright_2019_Advances-in-Domain-Adaptation-Theory.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\3SUYFDS5\\Copyright_2019_Advances-in-Domain-Adaptation-Theory.pdf:application/pdf;Front-matter_2019_Advances-in-Domain-Adaptation-Theory.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\UYCDRZPP\\Front-matter_2019_Advances-in-Domain-Adaptation-Theory.pdf:application/pdf;Index_2019_Advances-in-Domain-Adaptation-Theory.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\EFIQ4GKM\\Index_2019_Advances-in-Domain-Adaptation-Theory.pdf:application/pdf;Introduction_2019_Advances-in-Domain-Adaptation-Theory.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\IIIFJ476\\Introduction_2019_Advances-in-Domain-Adaptation-Theory.pdf:application/pdf;Notations_2019_Advances-in-Domain-Adaptation-Theory.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\AFKUP3TN\\Notations_2019_Advances-in-Domain-Adaptation-Theory.pdf:application/pdf;References_2019_Advances-in-Domain-Adaptation-Theory.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\ZWU36YD4\\References_2019_Advances-in-Domain-Adaptation-Theory.pdf:application/pdf},
}

@inproceedings{chen_representation_2021,
	title = {Representation {Subspace} {Distance} for {Domain} {Adaptation} {Regression}},
	url = {https://proceedings.mlr.press/v139/chen21u.html},
	abstract = {Regression, as a counterpart to classification, is a major paradigm with a wide range of applications. Domain adaptation regression extends it by generalizing a regressor from a labeled source domain to an unlabeled target domain. Existing domain adaptation regression methods have achieved positive results limited only to the shallow regime. A question arises: Why learning invariant representations in the deep regime less pronounced? A key finding of this paper is that classification is robust to feature scaling but regression is not, and aligning the distributions of deep representations will alter feature scale and impede domain adaptation regression. Based on this finding, we propose to close the domain gap through orthogonal bases of the representation spaces, which are free from feature scaling. Inspired by Riemannian geometry of Grassmann manifold, we define a geometrical distance over representation subspaces and learn deep transferable representations by minimizing it. To avoid breaking the geometrical properties of deep representations, we further introduce the bases mismatch penalization to match the ordering of orthogonal bases across representation subspaces. Our method is evaluated on three domain adaptation regression benchmarks, two of which are introduced in this paper. Our method outperforms the state-of-the-art methods significantly, forming early positive results in the deep regime.},
	language = {en},
	urldate = {2022-09-09},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Xinyang and Wang, Sinan and Wang, Jianmin and Long, Mingsheng},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	keywords = {domain adaptation, notion},
	pages = {1749--1759},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\RCAUX7B5\\Chen et al. - 2021 - Representation Subspace Distance for Domain Adapta.pdf:application/pdf;Supplementary PDF:C\:\\Users\\bahi_\\Zotero\\storage\\MDSQM8P6\\Chen et al. - 2021 - Representation Subspace Distance for Domain Adapta.pdf:application/pdf},
}

@misc{maheshwari_understanding_2021,
	title = {Understanding {Domain} {Adaptation}},
	url = {https://towardsdatascience.com/understanding-domain-adaptation-5baa723ac71f},
	abstract = {Learn how to design a deep learning framework enabling them for domain adaptation},
	language = {en},
	urldate = {2022-09-09},
	journal = {Medium},
	author = {Maheshwari, Harsh},
	month = sep,
	year = {2021},
	keywords = {domain adaptation, notion},
	file = {Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\4Z9XC9ER\\understanding-domain-adaptation-5baa723ac71f.html:text/html},
}

@misc{venkateswara_deep_2017,
	title = {Deep {Hashing} {Network} for {Unsupervised} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1706.07522},
	abstract = {In recent years, deep neural networks have emerged as a dominant machine learning tool for a wide variety of application domains. However, training a deep neural network requires a large amount of labeled data, which is an expensive process in terms of time, labor and human expertise. Domain adaptation or transfer learning algorithms address this challenge by leveraging labeled data in a different, but related source domain, to develop a model for the target domain. Further, the explosive growth of digital data has posed a fundamental challenge concerning its storage and retrieval. Due to its storage and retrieval efﬁciency, recent years have witnessed a wide application of hashing in a variety of computer vision applications. In this paper, we ﬁrst introduce a new dataset, Ofﬁce-Home, to evaluate domain adaptation algorithms. The dataset contains images of a variety of everyday objects from multiple domains. We then propose a novel deep learning framework that can exploit labeled source data and unlabeled target data to learn informative hash codes, to accurately classify unseen target data. To the best of our knowledge, this is the ﬁrst research effort to exploit the feature learning capabilities of deep neural networks to learn representative hash codes to address the domain adaptation problem. Our extensive empirical studies on multiple transfer tasks corroborate the usefulness of the framework in learning efﬁcient hash codes which outperform existing competitive baselines for unsupervised domain adaptation.},
	language = {en},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Venkateswara, Hemanth and Eusebio, Jose and Chakraborty, Shayok and Panchanathan, Sethuraman},
	month = jun,
	year = {2017},
	note = {arXiv:1706.07522 [cs]},
	keywords = {domain adaptation, notion},
	file = {Venkateswara et al. - 2017 - Deep Hashing Network for Unsupervised Domain Adapt.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\R3RMT52Q\\Venkateswara et al. - 2017 - Deep Hashing Network for Unsupervised Domain Adapt.pdf:application/pdf},
}

@misc{asadulaev_multi-step_2022,
	title = {Multi-step domain adaptation by adversarial attack to \${\textbackslash}mathcal\{{H}\} {\textbackslash}{Delta} {\textbackslash}mathcal\{{H}\}\$-divergence},
	url = {http://arxiv.org/abs/2207.08948},
	abstract = {Adversarial examples are transferable between different models. In our paper, we propose to use this property for multi-step domain adaptation. In unsupervised domain adaptation settings, we demonstrate that replacing the source domain with adversarial examples to H∆H-divergence can improve source classiﬁer accuracy on the target domain. Our method can be connected to most domain adaptation techniques. We conducted a range of experiments and achieved improvement in accuracy on Digits and OfﬁceHome datasets.},
	language = {en},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Asadulaev, Arip and Panfilov, Alexander and Filchenkov, Andrey},
	month = jul,
	year = {2022},
	note = {arXiv:2207.08948 [cs]},
	keywords = {domain adaptation, notion},
	file = {Asadulaev et al. - 2022 - Multi-step domain adaptation by adversarial attack.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\M87CSEQX\\Asadulaev et al. - 2022 - Multi-step domain adaptation by adversarial attack.pdf:application/pdf},
}

@inproceedings{long_conditional_2018,
	title = {Conditional {Adversarial} {Domain} {Adaptation}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/ab88b15733f543179858600245108dd8-Abstract.html},
	abstract = {Adversarial learning has been embedded into deep networks to learn disentangled and transferable representations for domain adaptation. Existing adversarial domain adaptation methods may struggle to align different domains of multimodal distributions that are native in classification problems. In this paper, we present conditional adversarial domain adaptation, a principled framework that conditions the adversarial adaptation models on discriminative information conveyed in the classifier predictions. Conditional domain adversarial networks (CDANs) are designed with two novel conditioning strategies: multilinear conditioning that captures the cross-covariance between feature representations and classifier predictions to improve the discriminability, and entropy conditioning that controls the uncertainty of classifier predictions to guarantee the transferability. Experiments testify that the proposed approach exceeds the state-of-the-art results on five benchmark datasets.},
	urldate = {2022-09-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Long, Mingsheng and CAO, ZHANGJIE and Wang, Jianmin and Jordan, Michael I},
	year = {2018},
	keywords = {domain adaptation, notion},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\5YYVUNF9\\Long et al. - 2018 - Conditional Adversarial Domain Adaptation.pdf:application/pdf},
}

@inproceedings{germain_pac-bayesian_2013,
	title = {A {PAC}-{Bayesian} {Approach} for {Domain} {Adaptation} with {Specialization} to {Linear} {Classifiers}},
	url = {https://proceedings.mlr.press/v28/germain13.html},
	abstract = {We provide a first PAC-Bayesian analysis for domain adaptation (DA) which arises when the learning and test distributions differ. It relies on a novel distribution pseudodistance based on a disagreement averaging. Using this measure, we derive a PAC-Bayesian DA bound for the stochastic Gibbs classifier. This bound has the advantage of being directly optimizable for any hypothesis space. We specialize it to linear classifiers, and design a learning algorithm which shows interesting results on a synthetic problem and on a popular sentiment annotation task. This opens the door to tackling DA tasks by making use of all the PAC-Bayesian tools.},
	language = {en},
	urldate = {2022-09-12},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Germain, Pascal and Habrard, Amaury and Laviolette, François and Morvant, Emilie},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	keywords = {domain adaptation, notion},
	pages = {738--746},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\NMCTN2TI\\Germain et al. - 2013 - A PAC-Bayesian Approach for Domain Adaptation with.pdf:application/pdf},
}

@inproceedings{ben-david_impossibility_2010,
	title = {Impossibility {Theorems} for {Domain} {Adaptation}},
	url = {https://proceedings.mlr.press/v9/david10a.html},
	abstract = {The domain adaptation problem in machine learning occurs when the test data generating distribution differs from the one that generates the training data. It is clear that the success of learning under such circumstances depends on similarities between the two data distributions. We study assumptions about the relationship between the two distributions that one needed for domain adaptation learning to succeed.  We analyze the assumptions in an agnostic PAC-style learning model for a the setting in which the learner can access a labeled training data sample and an unlabeled sample generated by the test data distribution. We focus on three assumptions: (i) Similarity between the unlabeled distributions, (ii) Existence of a classifier in the hypothesis class with low error on both training and testing distributions, and (iii) The covariate shift assumption. I.e., the assumption that the conditioned label distribution (for each data point) is the same for both the training and test distributions.  We show that without either assumption (i) or (ii), the combination of the remaining assumptions is not sufficient to guarantee successful learning. Our negative results hold with respect to any domain adaptation learning algorithm, as long as it does not have access to target labeled examples.  In particular, we provide formal proofs that the popular covariate shift assumption is rather weak and does not relieve the necessity of the other assumptions.  We also discuss the intuitively appealing paradigm of reweighing the labeled training sample according to the target unlabeled distribution. We show that, somewhat counter intuitively, that paradigm cannot be trusted in the following sense. There are DA tasks that are indistinguishable, as far as the input training data goes, but in which reweighing leads to significant improvement in one task, while causing dramatic deterioration of the learning success in the other.},
	language = {en},
	urldate = {2022-09-12},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Ben-David, Shai and Lu, Tyler and Luu, Teresa and Pal, David},
	month = mar,
	year = {2010},
	note = {ISSN: 1938-7228},
	keywords = {domain adaptation, notion},
	pages = {129--136},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\3P4PYARE\\David et al. - 2010 - Impossibility Theorems for Domain Adaptation.pdf:application/pdf},
}

@inproceedings{long_deep_2017,
	title = {Deep {Transfer} {Learning} with {Joint} {Adaptation} {Networks}},
	url = {https://proceedings.mlr.press/v70/long17a.html},
	abstract = {Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets.},
	language = {en},
	urldate = {2022-09-12},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	keywords = {domain adaptation, notion},
	pages = {2208--2217},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\NVYCU7L3\\Long et al. - 2017 - Deep Transfer Learning with Joint Adaptation Netwo.pdf:application/pdf},
}

@misc{shen_wasserstein_2018,
	title = {Wasserstein {Distance} {Guided} {Representation} {Learning} for {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1707.01217},
	abstract = {Domain adaptation aims at generalizing a high-performance learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation is to learn domain invariant feature representations while the learned representations should also be discriminative in prediction. To learn such representations, domain adaptation frameworks usually include a domain invariant representation learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classiﬁcation. Inspired by Wasserstein GAN, in this paper we propose a novel approach to learn domain invariant feature representations, namely Wasserstein Distance Guided Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by the domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical studies on common sentiment and image classiﬁcation adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation learning approaches.},
	language = {en},
	urldate = {2022-09-12},
	publisher = {arXiv},
	author = {Shen, Jian and Qu, Yanru and Zhang, Weinan and Yu, Yong},
	month = mar,
	year = {2018},
	note = {arXiv:1707.01217 [cs, stat]},
	keywords = {domain adaptation, optimal transport, notion},
	file = {Shen et al. - 2018 - Wasserstein Distance Guided Representation Learnin.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\5U3YAYXN\\Shen et al. - 2018 - Wasserstein Distance Guided Representation Learnin.pdf:application/pdf},
}

@inproceedings{liang_we_2020,
	title = {Do {We} {Really} {Need} to {Access} the {Source} {Data}? {Source} {Hypothesis} {Transfer} for {Unsupervised} {Domain} {Adaptation}},
	shorttitle = {Do {We} {Really} {Need} to {Access} the {Source} {Data}?},
	url = {https://proceedings.mlr.press/v119/liang20a.html},
	abstract = {Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. This work tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems. We propose a simple yet generic representation learning framework, named {\textbackslash}emph\{Source HypOthesis Transfer\} (SHOT). SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.},
	language = {en},
	urldate = {2022-09-12},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liang, Jian and Hu, Dapeng and Feng, Jiashi},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	keywords = {domain adaptation, test-time, notion},
	pages = {6028--6039},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\HNZ9IUV3\\Liang et al. - 2020 - Do We Really Need to Access the Source Data Sourc.pdf:application/pdf},
}

@misc{shu_dirt-t_2018,
	title = {A {DIRT}-{T} {Approach} to {Unsupervised} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1802.08735},
	doi = {10.48550/arXiv.1802.08735},
	abstract = {Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin \& Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks.},
	urldate = {2022-09-13},
	publisher = {arXiv},
	author = {Shu, Rui and Bui, Hung H. and Narui, Hirokazu and Ermon, Stefano},
	month = mar,
	year = {2018},
	note = {arXiv:1802.08735 [cs, stat]},
	keywords = {domain adaptation, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\UMC95ZTR\\Shu et al. - 2018 - A DIRT-T Approach to Unsupervised Domain Adaptatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\BF8TNAB4\\1802.html:text/html},
}

@misc{tzeng_adversarial_2017,
	title = {Adversarial {Discriminative} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1702.05464},
	doi = {10.48550/arXiv.1702.05464},
	abstract = {Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance. Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts. Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss. We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches. We propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.},
	urldate = {2022-09-13},
	publisher = {arXiv},
	author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
	month = feb,
	year = {2017},
	note = {arXiv:1702.05464 [cs]},
	keywords = {domain adaptation, adversarial, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\6LACZ9H9\\Tzeng et al. - 2017 - Adversarial Discriminative Domain Adaptation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\IJ2FX5KY\\1702.html:text/html},
}

@inproceedings{sun_deep_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Deep {CORAL}: {Correlation} {Alignment} for {Deep} {Domain} {Adaptation}},
	isbn = {978-3-319-49409-8},
	shorttitle = {Deep {CORAL}},
	doi = {10.1007/978-3-319-49409-8_35},
	abstract = {Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL [18] is a simple unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance. Our code is available at: https://github.com/VisionLearningGroup/CORAL.},
	language = {en},
	booktitle = {{ECCV} 2016 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Sun, Baochen and Saenko, Kate},
	editor = {Hua, Gang and Jégou, Hervé},
	year = {2016},
	keywords = {domain adaptation, notion},
	pages = {443--450},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\BSLMF5LY\\Sun and Saenko - 2016 - Deep CORAL Correlation Alignment for Deep Domain .pdf:application/pdf},
}

@inproceedings{hoffman_cycada_2018,
	title = {{CyCADA}: {Cycle}-{Consistent} {Adversarial} {Domain} {Adaptation}},
	shorttitle = {{CyCADA}},
	url = {https://proceedings.mlr.press/v80/hoffman18a.html},
	abstract = {Domain adaptation is critical for success in new, unseen environments. Adversarial adaptation models have shown tremendous progress towards adapting to new environments by focusing either on discovering domain invariant representations or by mapping between unpaired image domains. While feature space methods are difficult to interpret and sometimes fail to capture pixel-level and low-level domain shifts, image space methods sometimes fail to incorporate high level semantic knowledge relevant for the end task. We propose a model which adapts between domains using both generative image space alignment and latent representation space alignment. Our approach, Cycle-Consistent Adversarial Domain Adaptation (CyCADA), guides transfer between domains according to a specific discriminatively trained task and avoids divergence by enforcing consistency of the relevant semantics before and after adaptation. We evaluate our method on a variety of visual recognition and prediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art performance for unsupervised adaptation from synthetic to real world driving domains.},
	language = {en},
	urldate = {2022-09-13},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	keywords = {domain adaptation, adversarial, cyclegan, gan, notion},
	pages = {1989--1998},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\UKP7E26Z\\Hoffman et al. - 2018 - CyCADA Cycle-Consistent Adversarial Domain Adapta.pdf:application/pdf;Supplementary PDF:C\:\\Users\\bahi_\\Zotero\\storage\\3TAQYGZH\\Hoffman et al. - 2018 - CyCADA Cycle-Consistent Adversarial Domain Adapta.pdf:application/pdf},
}

@misc{sun_unsupervised_2019,
	title = {Unsupervised {Domain} {Adaptation} through {Self}-{Supervision}},
	url = {http://arxiv.org/abs/1909.11825},
	doi = {10.48550/arXiv.1909.11825},
	abstract = {This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data. Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously. Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain. The presented objective is straightforward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.},
	urldate = {2022-09-13},
	publisher = {arXiv},
	author = {Sun, Yu and Tzeng, Eric and Darrell, Trevor and Efros, Alexei A.},
	month = sep,
	year = {2019},
	note = {arXiv:1909.11825 [cs, stat]},
	keywords = {domain adaptation, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\9SLIENXX\\Sun et al. - 2019 - Unsupervised Domain Adaptation through Self-Superv.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\W8FUZ3JW\\1909.html:text/html},
}

@misc{tzeng_simultaneous_2015,
	title = {Simultaneous {Deep} {Transfer} {Across} {Domains} and {Tasks}},
	url = {http://arxiv.org/abs/1510.02192},
	doi = {10.48550/arXiv.1510.02192},
	abstract = {Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simultaneously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adaptation method offers empirical performance which exceeds previously published results on two standard benchmark visual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.},
	urldate = {2022-09-13},
	publisher = {arXiv},
	author = {Tzeng, Eric and Hoffman, Judy and Darrell, Trevor and Saenko, Kate},
	month = oct,
	year = {2015},
	note = {arXiv:1510.02192 [cs]},
	keywords = {domain adaptation, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\N8CYG296\\Tzeng et al. - 2015 - Simultaneous Deep Transfer Across Domains and Task.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\969ZL7L5\\1510.html:text/html},
}

@article{zhou_domain_2022,
	title = {Domain {Generalization}: {A} {Survey}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Domain {Generalization}},
	url = {http://arxiv.org/abs/2103.02503},
	doi = {10.1109/TPAMI.2022.3195549},
	abstract = {Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d.{\textasciitilde}assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Over the last ten years, research in DG has made great progress, leading to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, to name a few; DG has also been studied in various application areas including computer vision, speech recognition, natural language processing, medical imaging, and reinforcement learning. In this paper, for the first time a comprehensive literature review in DG is provided to summarize the developments over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other relevant fields like domain adaptation and transfer learning. Then, we conduct a thorough review into existing methods and theories. Finally, we conclude this survey with insights and discussions on future research directions.},
	urldate = {2022-09-13},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
	year = {2022},
	note = {arXiv:2103.02503 [cs]},
	keywords = {domain adaptation, domain generalization, notion},
	pages = {1--20},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\6N7LDKL9\\Zhou et al. - 2022 - Domain Generalization A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\AWAQ5MXX\\2103.html:text/html},
}

@inproceedings{courty_joint_2017,
	title = {Joint distribution optimal transportation for domain adaptation},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/0070d23b06b1486a538c0eaa45dd167a-Abstract.html},
	urldate = {2022-09-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Courty, Nicolas and Flamary, Rémi and Habrard, Amaury and Rakotomamonjy, Alain},
	year = {2017},
	keywords = {domain adaptation, notion},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\7N4YPGGV\\Courty et al. - 2017 - Joint distribution optimal transportation for doma.pdf:application/pdf},
}

@inproceedings{yang_fda_2020,
	address = {Seattle, WA, USA},
	title = {{FDA}: {Fourier} {Domain} {Adaptation} for {Semantic} {Segmentation}},
	isbn = {978-1-72817-168-5},
	shorttitle = {{FDA}},
	url = {https://ieeexplore.ieee.org/document/9157228/},
	doi = {10.1109/CVPR42600.2020.00414},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yang, Yanchao and Soatto, Stefano},
	month = jun,
	year = {2020},
	keywords = {domain adaptation, semantic segmentation, notion},
	pages = {4084--4094},
	file = {Yang and Soatto - 2020 - FDA Fourier Domain Adaptation for Semantic Segmen.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\BVVDBD6X\\Yang and Soatto - 2020 - FDA Fourier Domain Adaptation for Semantic Segmen.pdf:application/pdf},
}

@misc{zhang_prototypical_2021,
	title = {Prototypical {Pseudo} {Label} {Denoising} and {Target} {Structure} {Learning} for {Domain} {Adaptive} {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2101.10979},
	doi = {10.48550/arXiv.2101.10979},
	abstract = {Self-training is a competitive approach in domain adaptive segmentation, which trains the network with the pseudo labels on the target domain. However inevitably, the pseudo labels are noisy and the target features are dispersed due to the discrepancy between source and target domains. In this paper, we rely on representative prototypes, the feature centroids of classes, to address the two issues for unsupervised domain adaptation. In particular, we take one step further and exploit the feature distances from prototypes that provide richer information than mere prototypes. Specifically, we use it to estimate the likelihood of pseudo labels to facilitate online correction in the course of training. Meanwhile, we align the prototypical assignments based on relative feature distances for two different views of the same target, producing a more compact target feature space. Moreover, we find that distilling the already learned knowledge to a self-supervised pretrained model further boosts the performance. Our method shows tremendous performance advantage over state-of-the-art methods. We will make the code publicly available.},
	urldate = {2022-09-22},
	publisher = {arXiv},
	author = {Zhang, Pan and Zhang, Bo and Zhang, Ting and Chen, Dong and Wang, Yong and Wen, Fang},
	month = jan,
	year = {2021},
	note = {arXiv:2101.10979 [cs]},
	keywords = {domain adaptation, semantic segmentation, prototypes, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\43BQ5QYR\\Zhang et al. - 2021 - Prototypical Pseudo Label Denoising and Target Str.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\4B9A9RWI\\2101.html:text/html},
}

@inproceedings{yue_prototypical_2021,
	address = {Nashville, TN, USA},
	title = {Prototypical {Cross}-domain {Self}-supervised {Learning} for {Few}-shot {Unsupervised} {Domain} {Adaptation}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577429/},
	doi = {10.1109/CVPR46437.2021.01362},
	abstract = {Unsupervised Domain Adaptation (UDA) transfers predictive models from a fully-labeled source domain to an unlabeled target domain. In some applications, however, it is expensive even to collect labels in the source domain, making most previous works impractical. To cope with this problem, recent work performed instance-wise cross-domain self-supervised learning, followed by an additional ﬁne-tuning stage. However, the instance-wise selfsupervised learning only learns and aligns low-level discriminative features. In this paper, we propose an endto-end Prototypical Cross-domain Self-Supervised Learning (PCS) framework for Few-shot Unsupervised Domain Adaptation (FUDA)1. PCS not only performs cross-domain low-level feature alignment, but it also encodes and aligns semantic structures in the shared embedding space across domains. Our framework captures category-wise semantic structures of the data by in-domain prototypical contrastive learning; and performs feature alignment through cross-domain prototypical self-supervision. Compared with state-of-the-art methods, PCS improves the mean classiﬁcation accuracy over different domain pairs on FUDA by 10.5\%, 3.5\%, 9.0\%, and 13.2\% on Ofﬁce, Ofﬁce-Home, VisDA-2017, and DomainNet, respectively.},
	language = {en},
	urldate = {2022-09-22},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yue, Xiangyu and Zheng, Zangwei and Zhang, Shanghang and Gao, Yang and Darrell, Trevor and Keutzer, Kurt and Vincentelli, Alberto Sangiovanni},
	month = jun,
	year = {2021},
	keywords = {domain adaptation, few-shot, prototypes, notion},
	pages = {13829--13839},
	file = {Yue et al. - 2021 - Prototypical Cross-domain Self-supervised Learning.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\S6UN6WYE\\Yue et al. - 2021 - Prototypical Cross-domain Self-supervised Learning.pdf:application/pdf},
}

@inproceedings{zhao_multi-source_2019,
	title = {Multi-source {Domain} {Adaptation} for {Semantic} {Segmentation}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/db9ad56c71619aeed9723314d1456037-Abstract.html},
	abstract = {Simulation-to-real domain adaptation for semantic segmentation has been actively studied for various applications such as autonomous driving. Existing methods mainly focus on a single-source setting, which cannot easily handle a more practical scenario of multiple sources with different distributions. In this paper, we propose to investigate multi-source domain adaptation for semantic segmentation. Specifically, we design a novel framework, termed Multi-source Adversarial Domain Aggregation Network (MADAN), which can be trained in an end-to-end manner. First, we generate an adapted domain for each source with dynamic semantic consistency while aligning at the pixel-level cycle-consistently towards the target. Second, we propose sub-domain aggregation discriminator and cross-domain cycle discriminator to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and target domain while training the segmentation network. Extensive experiments from synthetic GTA and SYNTHIA to real Cityscapes and BDDS datasets demonstrate that the proposed MADAN model outperforms state-of-the-art approaches.  Our source code is released at: https://github.com/Luodian/MADAN.},
	urldate = {2022-09-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhao, Sicheng and Li, Bo and Yue, Xiangyu and Gu, Yang and Xu, Pengfei and Hu, Runbo and Chai, Hua and Keutzer, Kurt},
	year = {2019},
	keywords = {domain adaptation, semantic segmentation, multi-source, notion},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\2N6BA59D\\Zhao et al. - 2019 - Multi-source Domain Adaptation for Semantic Segmen.pdf:application/pdf},
}

@article{zhao_applications_2021,
	title = {Applications of {Unsupervised} {Deep} {Transfer} {Learning} to {Intelligent} {Fault} {Diagnosis}: {A} {Survey} and {Comparative} {Study}},
	volume = {70},
	issn = {0018-9456, 1557-9662},
	shorttitle = {Applications of {Unsupervised} {Deep} {Transfer} {Learning} to {Intelligent} {Fault} {Diagnosis}},
	url = {http://arxiv.org/abs/1912.12528},
	doi = {10.1109/TIM.2021.3116309},
	abstract = {Recent progress on intelligent fault diagnosis (IFD) has greatly depended on deep representation learning and plenty of labeled data. However, machines often operate with various working conditions or the target task has different distributions with the collected data used for training (the domain shift problem). Besides, the newly collected test data in the target domain are usually unlabeled, leading to unsupervised deep transfer learning based (UDTL-based) IFD problem. Although it has achieved huge development, a standard and open source code framework as well as a comparative study for UDTL-based IFD are not yet established. In this paper, we construct a new taxonomy and perform a comprehensive review of UDTL-based IFD according to different tasks. Comparative analysis of some typical methods and datasets reveals some open and essential issues in UDTL-based IFD which are rarely studied, including transferability of features, influence of backbones, negative transfer, physical priors, etc. To emphasize the importance and reproducibility of UDTL-based IFD, the whole test framework will be released to the research community to facilitate future research. In summary, the released framework and comparative study can serve as an extended interface and basic results to carry out new studies on UDTL-based IFD. The code framework is available at {\textbackslash}url\{https://github.com/ZhaoZhibin/UDTL\}.},
	urldate = {2022-09-22},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Zhao, Zhibin and Zhang, Qiyang and Yu, Xiaolei and Sun, Chuang and Wang, Shibin and Yan, Ruqiang and Chen, Xuefeng},
	year = {2021},
	note = {arXiv:1912.12528 [cs, eess]},
	keywords = {domain adaptation, fault diagnosis, notion},
	pages = {1--28},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\SC5VSHDA\\Zhao et al. - 2021 - Applications of Unsupervised Deep Transfer Learnin.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\2VFCH78A\\1912.html:text/html},
}

@article{yan_knowledge_2020,
	title = {Knowledge {Transfer} for {Rotary} {Machine} {Fault} {Diagnosis}},
	volume = {20},
	issn = {1558-1748},
	doi = {10.1109/JSEN.2019.2949057},
	abstract = {This paper intends to provide an overview on recent development of knowledge transfer for rotary machine fault diagnosis (RMFD) by using different transfer learning techniques. After brief introduction of parameter-based, instance-based, feature-based and relevance-based knowledge transfer, the applications of knowledge transfer in RMFD are summarized from four categories: transfer between multiple working conditions, transfer between multiple locations, transfer between multiple machines, and transfer between multiple fault types. Case studies on four datasets including gears, bearing, and motor faults verified effectiveness of knowledge transfer on improving diagnostic accuracy. Meanwhile, research trends on transfer learning in the field of RMFD are discussed.},
	number = {15},
	journal = {IEEE Sensors Journal},
	author = {Yan, Ruqiang and Shen, Fei and Sun, Chuang and Chen, Xuefeng},
	month = aug,
	year = {2020},
	note = {Conference Name: IEEE Sensors Journal},
	keywords = {domain adaptation, fault diagnosis, notion},
	pages = {8374--8393},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\7RLPCVNS\\Yan et al. - 2020 - Knowledge Transfer for Rotary Machine Fault Diagno.pdf:application/pdf},
}

@inproceedings{mei_instance_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Instance {Adaptive} {Self}-training for {Unsupervised} {Domain} {Adaptation}},
	isbn = {978-3-030-58574-7},
	doi = {10.1007/978-3-030-58574-7_25},
	abstract = {The divergence between labeled training data and unlabeled testing data is a significant challenge for recent deep learning models. Unsupervised domain adaptation (UDA) attempts to solve such a problem. Recent works show that self-training is a powerful approach to UDA. However, existing methods have difficulty in balancing scalability and performance. In this paper, we propose an instance adaptive self-training framework for UDA on the task of semantic segmentation. To effectively improve the quality of pseudo-labels, we develop a novel pseudo-label generation strategy with an instance adaptive selector. Besides, we propose the region-guided regularization to smooth the pseudo-label region and sharpen the non-pseudo-label region. Our method is so concise and efficient that it is easy to be generalized to other unsupervised domain adaptation methods. Experiments on ‘GTA5 to Cityscapes’ and ‘SYNTHIA to Cityscapes’ demonstrate the superior performance of our approach compared with the state-of-the-art methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Mei, Ke and Zhu, Chuang and Zou, Jiaqi and Zhang, Shanghang},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	keywords = {domain adaptation, semantic segmentation, notion, self-training},
	pages = {415--430},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\DP6HWXX3\\Mei et al. - 2020 - Instance Adaptive Self-training for Unsupervised D.pdf:application/pdf},
}

@article{dif_transfer_2022,
	title = {Transfer learning from synthetic labels for histopathological images classification},
	volume = {52},
	issn = {0924-669X, 1573-7497},
	url = {https://link.springer.com/10.1007/s10489-021-02425-z},
	doi = {10.1007/s10489-021-02425-z},
	abstract = {This study introduces a new strategy that combines unsupervised learning (clustering) and transfer learning. Clustering methods are employed to generate synthetic labels for the source dataset (ICAR-2018). The generated dataset is then used for transfer learning to other histopathological datasets (KimiaPath960, CRC, Biomaging−2015, Breakhis, and Lymphoma). The comparative study based on two clustering algorithms (K-means and multi-objective clustering stream) demonstrates the efficiency of MOC-Stream. The generated synthetic histopathological dataset by this clustering algorithm outperformed the original labeled dataset and the imageNet models in transfer learning.},
	language = {en},
	number = {1},
	urldate = {2022-09-26},
	journal = {Applied Intelligence},
	author = {Dif, Nassima and Attaoui, Mohammed Oualid and Elberrichi, Zakaria and Lebbah, Mustapha and Azzag, Hanene},
	month = jan,
	year = {2022},
	keywords = {notion},
	pages = {358--377},
	file = {Dif et al. - 2022 - Transfer learning from synthetic labels for histop.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\JH3SHIZ2\\Dif et al. - 2022 - Transfer learning from synthetic labels for histop.pdf:application/pdf},
}

@misc{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://arxiv.org/abs/1411.1792},
	doi = {10.48550/arXiv.1411.1792},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
	urldate = {2022-09-27},
	publisher = {arXiv},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	month = nov,
	year = {2014},
	note = {arXiv:1411.1792 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\5GMZB64F\\Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\PG5AP52Z\\1411.html:text/html},
}

@inproceedings{arora_generalization_2017,
	title = {Generalization and {Equilibrium} in {Generative} {Adversarial} {Nets} ({GANs})},
	url = {https://proceedings.mlr.press/v70/arora17a.html},
	abstract = {It is shown that training of generative adversarial network (GAN) may not have good generalization properties; e.g., training may appear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an approximate pure equilibrium exists in the discriminator/generator game for a natural training objective (Wasserstein) when generator capacity and training set sizes are moderate. This existence of equilibrium inspires MIX+GAN protocol, which can be combined with any existing GAN training, and empirically shown to improve some of them.},
	language = {en},
	urldate = {2022-09-27},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arora, Sanjeev and Ge, Rong and Liang, Yingyu and Ma, Tengyu and Zhang, Yi},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	keywords = {notion},
	pages = {224--232},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\E9UBCI6K\\Arora et al. - 2017 - Generalization and Equilibrium in Generative Adver.pdf:application/pdf;Supplementary PDF:C\:\\Users\\bahi_\\Zotero\\storage\\WPI77Y97\\Arora et al. - 2017 - Generalization and Equilibrium in Generative Adver.pdf:application/pdf},
}

@article{sugiyama_covariate_2007,
	title = {Covariate {Shift} {Adaptation} by {Importance} {Weighted} {Cross} {Validation}},
	volume = {8},
	abstract = {A common assumption in supervised learning is that the input points in the training set follow the same probability distribution as the input points that will be given in the future test phase. However, this assumption is not satisﬁed, for example, when the outside of the training region is extrapolated. The situation where the training input points and test input points follow different distributions while the conditional distribution of output values given input points is unchanged is called the covariate shift. Under the covariate shift, standard model selection techniques such as cross validation do not work as desired since its unbiasedness is no longer maintained. In this paper, we propose a new method called importance weighted cross validation (IWCV), for which we prove its unbiasedness even under the covariate shift. The IWCV procedure is the only one that can be applied for unbiased classiﬁcation under covariate shift, whereas alternatives to IWCV exist for regression. The usefulness of our proposed method is illustrated by simulations, and furthermore demonstrated in the brain-computer interface, where strong non-stationarity effects can be seen between training and test sessions.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Sugiyama, Masashi and Krauledat, Matthias and Müller, Klaus-Robert},
	year = {2007},
	keywords = {notion},
	pages = {985--1005},
	file = {Sugiyama and Krauledat - Covariate Shift Adaptation by Importance Weighted .pdf:C\:\\Users\\bahi_\\Zotero\\storage\\R7DII5RM\\Sugiyama and Krauledat - Covariate Shift Adaptation by Importance Weighted .pdf:application/pdf},
}

@misc{hoffman_fcns_2016,
	title = {{FCNs} in the {Wild}: {Pixel}-level {Adversarial} and {Constraint}-based {Adaptation}},
	shorttitle = {{FCNs} in the {Wild}},
	url = {http://arxiv.org/abs/1612.02649},
	doi = {10.48550/arXiv.1612.02649},
	abstract = {Fully convolutional models for dense prediction have proven successful for a wide range of visual tasks. Such models perform well in a supervised setting, but performance can be surprisingly poor under domain shifts that appear mild to a human observer. For example, training on one city and testing on another in a different geographic region and/or weather condition may result in significantly degraded performance due to pixel-level distribution shift. In this paper, we introduce the first domain adaptive semantic segmentation method, proposing an unsupervised adversarial approach to pixel prediction problems. Our method consists of both global and category specific adaptation techniques. Global domain alignment is performed using a novel semantic segmentation network with fully convolutional domain adversarial learning. This initially adapted space then enables category specific adaptation through a generalization of constrained weak learning, with explicit transfer of the spatial layout from the source to the target domains. Our approach outperforms baselines across different settings on multiple large-scale datasets, including adapting across various real city environments, different synthetic sub-domains, from simulated to real environments, and on a novel large-scale dash-cam dataset.},
	urldate = {2022-09-27},
	publisher = {arXiv},
	author = {Hoffman, Judy and Wang, Dequan and Yu, Fisher and Darrell, Trevor},
	month = dec,
	year = {2016},
	note = {arXiv:1612.02649 [cs]
version: 1},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\6CVWJZ68\\Hoffman et al. - 2016 - FCNs in the Wild Pixel-level Adversarial and Cons.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\67G2NRFU\\1612.html:text/html},
}

@article{shimodaira_improving_2000,
	title = {Improving predictive inference under covariate shift by weighting the log-likelihood function},
	volume = {90},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375800001154},
	doi = {10.1016/S0378-3758(00)00115-4},
	abstract = {A class of predictive densities is derived by weighting the observed samples in maximizing the log-likelihood function. This approach is effective in cases such as sample surveys or design of experiments, where the observed covariate follows a different distribution than that in the whole population. Under misspecification of the parametric model, the optimal choice of the weight function is asymptotically shown to be the ratio of the density function of the covariate in the population to that in the observations. This is the pseudo-maximum likelihood estimation of sample surveys. The optimality is defined by the expected Kullback–Leibler loss, and the optimal weight is obtained by considering the importance sampling identity. Under correct specification of the model, however, the ordinary maximum likelihood estimate (i.e. the uniform weight) is shown to be optimal asymptotically. For moderate sample size, the situation is in between the two extreme cases, and the weight function is selected by minimizing a variant of the information criterion derived as an estimate of the expected loss. The method is also applied to a weighted version of the Bayesian predictive density. Numerical examples as well as Monte-Carlo simulations are shown for polynomial regression. A connection with the robust parametric estimation is discussed.},
	language = {en},
	number = {2},
	urldate = {2022-09-27},
	journal = {Journal of Statistical Planning and Inference},
	author = {Shimodaira, Hidetoshi},
	month = oct,
	year = {2000},
	keywords = {notion},
	pages = {227--244},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\32DGU5QB\\Shimodaira - 2000 - Improving predictive inference under covariate shi.pdf:application/pdf},
}

@inproceedings{zhang_domain_2013,
	title = {Domain {Adaptation} under {Target} and {Conditional} {Shift}},
	url = {https://proceedings.mlr.press/v28/zhang13d.html},
	abstract = {Let X denote the feature and Y the target. We consider domain adaptation under three possible scenarios: (1) the marginal P\_Y changes, while the conditional P\_X{\textbar}Y stays the same ({\textbackslash}it target shift), (2) the marginal P\_Y is fixed, while the conditional P\_X{\textbar}Y changes with certain constraints ({\textbackslash}it conditional shift), and (3) the marginal P\_Y changes, and the conditional P\_X{\textbar}Y changes with constraints ({\textbackslash}it generalized target shift). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to find the learning machine that works well on test data, and propose to estimate the weights or transformations by {\textbackslash}it reweighting or transforming training data to reproduce the covariate distribution on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and real-world datasets demonstrate the effectiveness of the proposed framework.},
	language = {en},
	urldate = {2022-09-27},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Kun and Schölkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},
	month = may,
	year = {2013},
	note = {ISSN: 1938-7228},
	keywords = {notion},
	pages = {819--827},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\MMEIGVKH\\Zhang et al. - 2013 - Domain Adaptation under Target and Conditional Shi.pdf:application/pdf},
}

@misc{bousmalis_domain_2016,
	title = {Domain {Separation} {Networks}},
	url = {http://arxiv.org/abs/1608.06019},
	doi = {10.48550/arXiv.1608.06019},
	abstract = {The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domain-invariant features. Inspired by work on private-shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state-of-the-art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.},
	urldate = {2022-09-28},
	publisher = {arXiv},
	author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
	month = aug,
	year = {2016},
	note = {arXiv:1608.06019 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\YBUSURJM\\Bousmalis et al. - 2016 - Domain Separation Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\LX98AFRS\\1608.html:text/html},
}

@article{wang_missing-class-robust_2021,
	title = {Missing-{Class}-{Robust} {Domain} {Adaptation} by {Unilateral} {Alignment}},
	volume = {68},
	issn = {1557-9948},
	doi = {10.1109/TIE.2019.2962438},
	abstract = {Domain adaptation aims at improving model performance by leveraging the learned knowledge in the source domain and transferring it to the target domain. Recently, domain adversarial methods have been particularly successful in alleviating the distribution shift between the source and the target domains. However, these methods assume an identical label space between the two domains. This assumption imposes a significant limitation for real applications since the target training set may not contain the complete set of classes. We demonstrate in this article that the performance of domain adversarial methods can be vulnerable to an incomplete target label space during training. To overcome this issue, we propose a two-stage unilateral alignment approach. The proposed methodology makes use of the interclass relationships of the source domain and aligns unilaterally the target to the source domain. The benefits of the proposed methodology are first evaluated on the modified national institute of standards and technology database (MNIST)→ MNIST-M adaptation task. The proposed methodology is also evaluated on a fault diagnosis task, where the problem of missing fault types in the target training dataset is common in practice. Both experiments demonstrate the effectiveness of the proposed methodology.},
	number = {1},
	journal = {IEEE Transactions on Industrial Electronics},
	author = {Wang, Qin and Michau, Gabriel and Fink, Olga},
	month = jan,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Industrial Electronics},
	keywords = {domain adaptation, fault diagnosis, notion},
	pages = {663--671},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\bahi_\\Zotero\\storage\\9E5YB983\\8949730.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\7MFSXGDQ\\Wang et al. - 2021 - Missing-Class-Robust Domain Adaptation by Unilater.pdf:application/pdf},
}

@misc{rombach_controlled_2022,
	title = {Controlled {Generation} of {Unseen} {Faults} for {Partial} and {Open}-{Partial} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2204.14068},
	doi = {10.48550/arXiv.2204.14068},
	abstract = {New operating conditions can result in a significant performance drop of fault diagnostics models due to the domain shift between the training and the testing data distributions. While several domain adaptation approaches have been proposed to overcome such domain shifts, their application is limited if the fault classes represented in the two domains are not the same. To enable a better transferability of the trained models between two different domains, particularly in setups where only the healthy data class is shared between the two domains, we propose a new framework for Partial and Open-Partial domain adaptation based on generating distinct fault signatures with a Wasserstein GAN. The main contribution of the proposed framework is the controlled synthetic fault data generation with two main distinct characteristics. Firstly, the proposed methodology enables to generate unobserved fault types in the target domain by having only access to the healthy samples in the target domain and faulty samples in the source domain. Secondly, the fault generation can be controlled to precisely generate distinct fault types and fault severity levels. The proposed method is especially suited in extreme domain adaption settings that are particularly relevant in the context of complex and safety-critical systems, where only one class is shared between the two domains. We evaluate the proposed framework on Partial as well as Open-Partial domain adaptation tasks on two bearing fault diagnostics case studies. Our experiments conducted in different label space settings showcase the versatility of the proposed framework. The proposed methodology provided superior results compared to other methods given large domain gaps.},
	urldate = {2022-09-29},
	publisher = {arXiv},
	author = {Rombach, Katharina and Michau, Dr Gabriel and Fink, Prof Dr Olga},
	month = jul,
	year = {2022},
	note = {arXiv:2204.14068 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\CFMUP6LA\\Rombach et al. - 2022 - Controlled Generation of Unseen Faults for Partial.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\8NJTQQCQ\\2204.html:text/html},
}

@inproceedings{rombach_improving_2020,
	title = {Improving {Generalization} of {Deep} {Fault} {Detection} {Models} in the {Presence} of {Mislabeled} {Data}},
	url = {http://arxiv.org/abs/2009.14606},
	doi = {10.1109/SMC42975.2020.9283002},
	abstract = {Mislabeled samples are ubiquitous in real-world datasets as rule-based or expert labeling is usually based on incorrect assumptions or subject to biased opinions. Neural networks can "memorize" these mislabeled samples and, as a result, exhibit poor generalization. This poses a critical issue in fault detection applications, where not only the training but also the validation datasets are prone to contain mislabeled samples. In this work, we propose a novel two-step framework for robust training with label noise. In the first step, we identify outliers (including the mislabeled samples) based on the update in the hypothesis space. In the second step, we propose different approaches to modifying the training data based on the identified outliers and a data augmentation technique. Contrary to previous approaches, we aim at finding a robust solution that is suitable for real-world applications, such as fault detection, where no clean, "noise-free" validation dataset is available. Under an approximate assumption about the upper limit of the label noise, we significantly improve the generalization ability of the model trained under massive label noise.},
	urldate = {2022-09-29},
	booktitle = {2020 {IEEE} {International} {Conference} on {Systems}, {Man}, and {Cybernetics} ({SMC})},
	author = {Rombach, Katharina and Michau, Gabriel and Fink, Olga},
	month = oct,
	year = {2020},
	note = {arXiv:2009.14606 [cs, stat]},
	keywords = {notion},
	pages = {3103--3110},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\MV43LEJ6\\Rombach et al. - 2020 - Improving Generalization of Deep Fault Detection M.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\43AQYDJA\\2009.html:text/html},
}

@inproceedings{zou_confidence_2019,
	title = {Confidence {Regularized} {Self}-{Training}},
	doi = {10.1109/ICCV.2019.00608},
	abstract = {Recent advances in domain adaptation show that deep self-training presents a powerful means for unsupervised domain adaptation. These methods often involve an iterative process of predicting on target domain and then taking the confident predictions as pseudo-labels for retraining. However, since pseudo-labels can be noisy, self-training can put overconfident label belief on wrong classes, leading to deviated solutions with propagated errors. To address the problem, we propose a confidence regularized self-training (CRST) framework, formulated as regularized self-training. Our method treats pseudo-labels as continuous latent variables jointly optimized via alternating optimization. We propose two types of confidence regularization: label regularization (LR) and model regularization (MR). CRST-LR generates soft pseudo-labels while CRST-MR encourages the smoothness on network output. Extensive experiments on image classification and semantic segmentation show that CRSTs outperform their non-regularized counterpart with state-of-the-art performance. The code and models of this work are available at https://github.com/yzou2/CRST.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zou, Yang and Yu, Zhiding and Liu, Xiaofeng and Kumar, B. V. K. Vijaya and Wang, Jinsong},
	month = oct,
	year = {2019},
	note = {ISSN: 2380-7504},
	keywords = {domain adaptation, semantic segmentation, notion, self-training},
	pages = {5981--5990},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\bahi_\\Zotero\\storage\\MVVGTRUQ\\9010413.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\BEJI9IRE\\Zou et al. - 2019 - Confidence Regularized Self-Training.pdf:application/pdf},
}

@article{zheng_rectifying_2021,
	title = {Rectifying {Pseudo} {Label} {Learning} via {Uncertainty} {Estimation} for {Domain} {Adaptive} {Semantic} {Segmentation}},
	volume = {129},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-020-01395-y},
	doi = {10.1007/s11263-020-01395-y},
	abstract = {This paper focuses on the unsupervised domain adaptation of transferring the knowledge from the source domain to the target domain in the context of semantic segmentation. Existing approaches usually regard the pseudo label as the ground truth to fully exploit the unlabeled target-domain data. Yet the pseudo labels of the target-domain data are usually predicted by the model trained on the source domain. Thus, the generated labels inevitably contain the incorrect prediction due to the discrepancy between the training domain and the test domain, which could be transferred to the final adapted model and largely compromises the training process. To overcome the problem, this paper proposes to explicitly estimate the prediction uncertainty during training to rectify the pseudo label learning for unsupervised semantic segmentation adaptation. Given the input image, the model outputs the semantic segmentation prediction as well as the uncertainty of the prediction. Specifically, we model the uncertainty via the prediction variance and involve the uncertainty into the optimization objective. To verify the effectiveness of the proposed method, we evaluate the proposed method on two prevalent synthetic-to-real semantic segmentation benchmarks, i.e., GTA5 \$\${\textbackslash}rightarrow \$\$Cityscapes and SYNTHIA \$\${\textbackslash}rightarrow \$\$Cityscapes, as well as one cross-city benchmark, i.e., Cityscapes \$\${\textbackslash}rightarrow \$\$Oxford RobotCar. We demonstrate through extensive experiments that the proposed approach (1) dynamically sets different confidence thresholds according to the prediction variance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves significant improvements over the conventional pseudo label learning and yields competitive performance on all three benchmarks.},
	language = {en},
	number = {4},
	urldate = {2022-10-03},
	journal = {International Journal of Computer Vision},
	author = {Zheng, Zhedong and Yang, Yi},
	month = apr,
	year = {2021},
	keywords = {domain adaptation, semantic segmentation, notion, uncertainty},
	pages = {1106--1120},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\E7HCIGRT\\Zheng and Yang - 2021 - Rectifying Pseudo Label Learning via Uncertainty E.pdf:application/pdf},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\NEMLWBSF\\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\Q4T8UUJJ\\1503.html:text/html},
}

@misc{zou_domain_2018,
	title = {Domain {Adaptation} for {Semantic} {Segmentation} via {Class}-{Balanced} {Self}-{Training}},
	url = {http://arxiv.org/abs/1810.07911},
	doi = {10.48550/arXiv.1810.07911},
	abstract = {Recent deep networks achieved state of the art performance on a variety of semantic segmentation tasks. Despite such progress, these models often face challenges in real world `wild tasks' where large difference between labeled training/source data and unseen test/target data exists. In particular, such difference is often referred to as `domain gap', and could cause significantly decreased performance which cannot be easily remedied by further increasing the representation power. Unsupervised domain adaptation (UDA) seeks to overcome such problem without target domain labels. In this paper, we propose a novel UDA framework based on an iterative self-training procedure, where the problem is formulated as latent variable loss minimization, and can be solved by alternatively generating pseudo labels on target data and re-training the model with these labels. On top of self-training, we also propose a novel class-balanced self-training framework to avoid the gradual dominance of large classes on pseudo-label generation, and introduce spatial priors to refine generated labels. Comprehensive experiments show that the proposed methods achieve state of the art semantic segmentation performance under multiple major UDA settings.},
	urldate = {2022-10-03},
	publisher = {arXiv},
	author = {Zou, Yang and Yu, Zhiding and Kumar, B. V. K. Vijaya and Wang, Jinsong},
	month = oct,
	year = {2018},
	note = {arXiv:1810.07911 [cs]},
	keywords = {domain adaptation, semantic segmentation, notion, self-training},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\P8VU7T7X\\Zou et al. - 2018 - Domain Adaptation for Semantic Segmentation via Cl.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\E6LMXTMV\\1810.html:text/html},
}

@inproceedings{tranheden_dacs_2021,
	address = {Waikoloa, HI, USA},
	title = {{DACS}: {Domain} {Adaptation} via {Cross}-domain {Mixed} {Sampling}},
	isbn = {978-1-66540-477-8},
	shorttitle = {{DACS}},
	url = {https://ieeexplore.ieee.org/document/9423032/},
	doi = {10.1109/WACV48630.2021.00142},
	abstract = {Semantic segmentation models based on convolutional neural networks have recently displayed remarkable performance for a multitude of applications. However, these models typically do not generalize well when applied on new domains, especially when going from synthetic to real data. In this paper we address the problem of unsupervised domain adaptation (UDA), which attempts to train on labelled data from one domain (source domain), and simultaneously learn from unlabelled data in the domain of interest (target domain). Existing methods have seen success by training on pseudo-labels for these unlabelled images. Multiple techniques have been proposed to mitigate low-quality pseudolabels arising from the domain shift, with varying degrees of success. We propose DACS: Domain Adaptation via Crossdomain mixed Sampling, which mixes images from the two domains along with the corresponding labels and pseudolabels. These mixed samples are then trained on, in addition to the labelled data itself. We demonstrate the effectiveness of our solution by achieving state-of-the-art results for GTA5 to Cityscapes, a common synthetic-to-real semantic segmentation benchmark for UDA.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {2021 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Tranheden, Wilhelm and Olsson, Viktor and Pinto, Juliano and Svensson, Lennart},
	month = jan,
	year = {2021},
	keywords = {domain adaptation, semantic segmentation, notion, self-training},
	pages = {1378--1388},
	file = {Tranheden et al. - 2021 - DACS Domain Adaptation via Cross-domain Mixed Sam.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\M47GWL74\\Tranheden et al. - 2021 - DACS Domain Adaptation via Cross-domain Mixed Sam.pdf:application/pdf},
}

@inproceedings{wen_bayesian_2019,
	address = {Macao, China},
	title = {Bayesian {Uncertainty} {Matching} for {Unsupervised} {Domain} {Adaptation}},
	isbn = {978-0-9992411-4-1},
	url = {https://www.ijcai.org/proceedings/2019/534},
	doi = {10.24963/ijcai.2019/534},
	abstract = {Domain adaptation is an important technique to alleviate performance degradation caused by domain shift, e.g., when training and test data come from different domains. Most existing deep adaptation methods focus on reducing domain shift by matching marginal feature distributions through deep transformations on the input features, due to the unavailability of target domain labels. We show that domain shift may still exist via label distribution shift at the classiﬁer, thus deteriorating model performances. To alleviate this issue, we propose an approximate joint distribution matching scheme by exploiting prediction uncertainty. Speciﬁcally, we use a Bayesian neural network to quantify prediction uncertainty of a classiﬁer. By imposing distribution matching on both features and labels (via uncertainty), label distribution mismatching in source and target data is effectively alleviated, encouraging the classiﬁer to produce consistent predictions across domains. We also propose a few techniques to improve our method by adaptively reweighting domain adaptation loss to achieve nontrivial distribution matching and stable training. Comparisons with state of the art unsupervised domain adaptation methods on three popular benchmark datasets demonstrate the superiority of our approach, especially on the effectiveness of alleviating negative transfer.},
	language = {en},
	urldate = {2022-10-04},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Wen, Jun and Zheng, Nenggan and Yuan, Junsong and Gong, Zhefeng and Chen, Changyou},
	month = aug,
	year = {2019},
	keywords = {notion, uncertainty},
	pages = {3849--3855},
	file = {Wen et al. - 2019 - Bayesian Uncertainty Matching for Unsupervised Dom.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\WLPQ4768\\Wen et al. - 2019 - Bayesian Uncertainty Matching for Unsupervised Dom.pdf:application/pdf},
}

@misc{han_unsupervised_2019,
	title = {Unsupervised {Domain} {Adaptation} via {Calibrating} {Uncertainties}},
	url = {http://arxiv.org/abs/1907.11202},
	doi = {10.48550/arXiv.1907.11202},
	abstract = {Unsupervised domain adaptation (UDA) aims at inferring class labels for unlabeled target domain given a related labeled source dataset. Intuitively, a model trained on source domain normally produces higher uncertainties for unseen data. In this work, we build on this assumption and propose to adapt from source to target domain via calibrating their predictive uncertainties. The uncertainty is quantified as the Renyi entropy, from which we propose a general Renyi entropy regularization (RER) framework. We further employ variational Bayes learning for reliable uncertainty estimation. In addition, calibrating the sample variance of network parameters serves as a plug-in regularizer for training. We discuss the theoretical properties of the proposed method and demonstrate its effectiveness on three domain-adaptation tasks.},
	urldate = {2022-10-04},
	publisher = {arXiv},
	author = {Han, Ligong and Zou, Yang and Gao, Ruijiang and Wang, Lezi and Metaxas, Dimitris},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11202 [cs, stat]},
	keywords = {domain adaptation, notion, uncertainty},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\ZBFDYE8U\\Han et al. - 2019 - Unsupervised Domain Adaptation via Calibrating Unc.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\LSD8FG6V\\1907.html:text/html},
}

@inproceedings{prabhu_active_2021,
	address = {Montreal, QC, Canada},
	title = {Active {Domain} {Adaptation} via {Clustering} {Uncertainty}-weighted {Embeddings}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9710350/},
	doi = {10.1109/ICCV48922.2021.00839},
	abstract = {Generalizing deep neural networks to new target domains is critical to their real-world utility. In practice, it may be feasible to get some target data labeled, but to be cost-effective it is desirable to select a maximally-informative subset via active learning (AL). We study the problem of AL under a domain shift, called Active Domain Adaptation (Active DA). We demonstrate how existing AL approaches based solely on model uncertainty or diversity sampling are less effective for Active DA. We propose Clustering Uncertainty-weighted Embeddings (CLUE), a novel label acquisition strategy for Active DA that performs uncertainty-weighted clustering to identify target instances for labeling that are both uncertain under the model and diverse in feature space. CLUE consistently outperforms competing label acquisition strategies for Active DA and AL across learning settings on 6 diverse domain shifts for image classification. Our code is available at https://github.com/virajprabhu/CLUE.},
	language = {en},
	urldate = {2022-10-04},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Prabhu, Viraj and Chandrasekaran, Arjun and Saenko, Kate and Hoffman, Judy},
	month = oct,
	year = {2021},
	keywords = {notion, uncertainty},
	pages = {8485--8494},
	file = {Prabhu et al. - 2021 - Active Domain Adaptation via Clustering Uncertaint.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\987YJTEE\\Prabhu et al. - 2021 - Active Domain Adaptation via Clustering Uncertaint.pdf:application/pdf},
}

@misc{da_costa_remaining_2019,
	title = {Remaining {Useful} {Lifetime} {Prediction} via {Deep} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1907.07480},
	doi = {10.48550/arXiv.1907.07480},
	abstract = {In Prognostics and Health Management (PHM) sufficient prior observed degradation data is usually critical for Remaining Useful Lifetime (RUL) prediction. Most previous data-driven prediction methods assume that training (source) and testing (target) condition monitoring data have similar distributions. However, due to different operating conditions, fault modes, noise and equipment updates distribution shift exists across different data domains. This shift reduces the performance of predictive models previously built to specific conditions when no observed run-to-failure data is available for retraining. To address this issue, this paper proposes a new data-driven approach for domain adaptation in prognostics using Long Short-Term Neural Networks (LSTM). We use a time window approach to extract temporal information from time-series data in a source domain with observed RUL values and a target domain containing only sensor information. We propose a Domain Adversarial Neural Network (DANN) approach to learn domain-invariant features that can be used to predict the RUL in the target domain. The experimental results show that the proposed method can provide more reliable RUL predictions under datasets with different operating conditions and fault modes. These results suggest that the proposed method offers a promising approach to performing domain adaptation in practical PHM applications.},
	urldate = {2022-10-04},
	publisher = {arXiv},
	author = {da Costa, Paulo R. de O. and Akcay, Alp and Zhang, Yingqian and Kaymak, Uzay},
	month = jul,
	year = {2019},
	note = {arXiv:1907.07480 [cs, stat]
version: 1},
	keywords = {domain adaptation, notion, prognostics, regression},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\ZJ3PF96X\\da Costa et al. - 2019 - Remaining Useful Lifetime Prediction via Deep Doma.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\A2XZKS6H\\1907.html:text/html},
}

@article{wagle_forward_2017,
	title = {Forward {Adaptive} {Transfer} of {Gaussian} {Process} {Regression}},
	volume = {14},
	issn = {2327-3097},
	url = {https://arc.aiaa.org/doi/10.2514/1.I010437},
	doi = {10.2514/1.I010437},
	language = {en},
	number = {4},
	urldate = {2022-10-04},
	journal = {Journal of Aerospace Information Systems},
	author = {Wagle, Neeti and Frew, Eric W.},
	month = apr,
	year = {2017},
	keywords = {notion, regression},
	pages = {214--231},
	file = {Wagle and Frew - 2017 - Forward Adaptive Transfer of Gaussian Process Regr.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\FML78XK2\\Wagle and Frew - 2017 - Forward Adaptive Transfer of Gaussian Process Regr.pdf:application/pdf},
}

@article{wen_data-driven_2021,
	title = {Data-driven remaining useful life prediction based on domain adaptation},
	volume = {7},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-690},
	doi = {10.7717/peerj-cs.690},
	abstract = {As an important part of prognostics and health management, remaining useful life (RUL) prediction can provide users and managers with system life information and improve the reliability of maintenance systems. Data-driven methods are powerful tools for RUL prediction because of their great modeling abilities. However, most current data-driven studies require large amounts of labeled training data and assume that the training data and test data follow similar distributions. In fact, the collected data are often variable due to different equipment operating conditions, fault modes, and noise distributions. As a result, the assumption that the training data and the test data obey the same distribution may not be valid. In response to the above problems, this paper proposes a data-driven framework with domain adaptability using a bidirectional gated recurrent unit (BGRU). The framework uses a domain-adversarial neural network (DANN) to implement transfer learning (TL) from the source domain to the target domain, which contains only sensor information. To verify the effectiveness of the proposed method, we analyze the IEEE PHM 2012 Challenge datasets and use them for verification. The experimental results show that the generalization ability of the model is effectively improved through the domain adaptation approach.},
	language = {en},
	urldate = {2022-10-04},
	journal = {PeerJ Computer Science},
	author = {Wen, Bin cheng and Xiao, Ming qing and Wang, Xue qi and Zhao, Xin and Li, Jian feng and Chen, Xin},
	month = sep,
	year = {2021},
	keywords = {notion, regression},
	pages = {e690},
	file = {Wen et al. - 2021 - Data-driven remaining useful life prediction based.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\7REP6F4W\\Wen et al. - 2021 - Data-driven remaining useful life prediction based.pdf:application/pdf},
}

@inproceedings{kandemir_asymmetric_2015,
	address = {Lille, France},
	title = {Asymmetric {Transfer} {Learning} with {Deep} {Gaussian} {Processes}},
	volume = {37},
	abstract = {We introduce a novel Gaussian process based Bayesian model for asymmetric transfer learning. We adopt a two-layer feed-forward deep Gaussian process as the task learner of source and target domains. The ﬁrst layer projects the data onto a separate non-linear manifold for each task. We perform knowledge transfer by projecting the target data also onto the source domain and linearly combining its representations on the source and target domain manifolds. Our approach achieves the state-of-the-art in a benchmark real-world image categorization task, and improves on it in cross-tissue tumor detection from histopathology tissue slide images.},
	language = {en},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	author = {Kandemir, Melih},
	year = {2015},
	keywords = {notion, regression},
	pages = {9},
	file = {Kandemir - Asymmetric Transfer Learning with Deep Gaussian Pr.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\SW4MTIKP\\Kandemir - Asymmetric Transfer Learning with Deep Gaussian Pr.pdf:application/pdf},
}

@article{che_domain_2020,
	title = {Domain adaptive deep belief network for rolling bearing fault diagnosis},
	volume = {143},
	issn = {0360-8352},
	url = {https://www.sciencedirect.com/science/article/pii/S0360835220301613},
	doi = {10.1016/j.cie.2020.106427},
	abstract = {As the essential components of rotating machines, rolling bearings always operate in variable working conditions and suffer from different failure modes. To address the issue of lacking substantial labeled samples in new working conditions, a domain adaptive deep belief network (DA-DBN) is proposed for rolling bearing fault diagnosis. Firstly, the DBN model is pre-trained by the labeled samples which are composed of raw vibration signals and their corresponding time domain and frequency domain indicators. Secondly, the domain adaption method in transfer learning is applied to calculate the multi-kernel maximum mean discrepancies (MK-MMD) between the known working condition data and new working condition data in multiple layers. Thus, the loss function composed of MK-MMD and classification error can be obtained, and back propagation algorithm is used to fine-tune model parameters. Finally, the datasets with five fault patterns are collected to evaluate the performance of the DA-DBN. The results demonstrate that the proposed DA-DBN can achieve more than 92\% fault classification accuracy under three noise levels; the average accuracy of fault classification under variable working conditions is 93.5\%, which is the highest compared with other models.},
	language = {en},
	urldate = {2022-10-05},
	journal = {Computers \& Industrial Engineering},
	author = {Che, Changchang and Wang, Huawei and Ni, Xiaomei and Fu, Qiang},
	month = may,
	year = {2020},
	keywords = {domain adaptation, Fault diagnosis, notion, Rolling bearing},
	pages = {106427},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\G3SGAKCL\\Che et al. - 2020 - Domain adaptive deep belief network for rolling be.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\32NYMLPY\\S0360835220301613.html:text/html},
}

@article{long_domain_2015,
	title = {Domain {Invariant} {Transfer} {Kernel} {Learning}},
	volume = {27},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2014.2373376},
	abstract = {Domain transfer learning generalizes a learning model across training data and testing data with different distributions. A general principle to tackle this problem is reducing the distribution difference between training data and testing data such that the generalization error can be bounded. Current methods typically model the sample distributions in input feature space, which depends on nonlinear feature mapping to embody the distribution discrepancy. However, this nonlinear feature space may not be optimal for the kernel-based learning machines. To this end, we propose a transfer kernel learning (TKL) approach to learn a domain-invariant kernel by directly matching source and target distributions in the reproducing kernel Hilbert space (RKHS). Specifically, we design a family of spectral kernels by extrapolating target eigensystem on source samples with Mercer's theorem. The spectral kernel minimizing the approximation error to the ground truth kernel is selected to construct domain-invariant kernel machines. Comprehensive experimental evidence on a large number of text categorization, image classification, and video event recognition datasets verifies the effectiveness and efficiency of the proposed TKL approach over several state-of-the-art methods.},
	number = {6},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Long, Mingsheng and Wang, Jianmin and Sun, Jiaguang and Yu, Philip S.},
	month = jun,
	year = {2015},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Approximation error, Eigenvalues and eigenfunctions, Hilbert space, image classification, kernel learning, notion, Nystrom method, Nyström method, Standards, Testing, text mining, Transfer learning, video recognition},
	pages = {1519--1532},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\HG6LGFAL\\Long et al. - 2015 - Domain Invariant Transfer Kernel Learning.pdf:application/pdf},
}

@article{wang_bi-adapting_2020,
	title = {Bi-adapting kernel learning for unsupervised domain adaptation},
	volume = {398},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231219310410},
	doi = {10.1016/j.neucom.2019.03.101},
	abstract = {Unsupervised domain adaptation aims to use labeled instances from a source domain to train a good learning model, which can classify unlabeled instances from a target domain as accurate as possible. The biggest challenge is that datasets from the source and target domains have different distributions, thus the general classification model trained on the source domain can not perform well on the target domain data. The classic methods solve this problem mainly by narrowing the distance between the source and target domains. Those methods, however, is not optimal since the nonlinear feature space may not match the kernel-based learning machine. In this paper, we design a new method called bi-adapt kernel learning (BAKL) to learn a domain-invariant kernel by transferring the source and target domains to each other simultaneously. Specifically, we derive the new source and target domain kernel matrix according to the Mercer’s theorem. The domain-invariant kernel machines are then constructed by minimizing the approximation error between the newly generated kernel matrices and the ground truth source domain kernel matrices. Experiments on benchmark tasks of text and object recognition demonstrate that it significantly improves classification accuracy compared to the state-of-art methods.},
	language = {en},
	urldate = {2022-10-05},
	journal = {Neurocomputing},
	author = {Wang, Zengmao and Xiao, Pan and Tu, Weiping and Du, Bo and Cheng, Yanxiang},
	month = jul,
	year = {2020},
	keywords = {notion, Text and object recognition, Unsupervised domain adaptation},
	pages = {547--554},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\3ID3U8G3\\Wang et al. - 2020 - Bi-adapting kernel learning for unsupervised domai.pdf:application/pdf},
}

@inproceedings{kim_unsupervised_2019,
	address = {Long Beach, CA, USA},
	title = {Unsupervised {Visual} {Domain} {Adaptation}: {A} {Deep} {Max}-{Margin} {Gaussian} {Process} {Approach}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Unsupervised {Visual} {Domain} {Adaptation}},
	url = {https://ieeexplore.ieee.org/document/8953535/},
	doi = {10.1109/CVPR.2019.00451},
	abstract = {For unsupervised domain adaptation, the target domain error can be provably reduced by having a shared input representation that makes the source and target domains indistinguishable from each other. Very recently it has been shown that it is not only critical to match the marginal input distributions, but also align the output class distributions. The latter can be achieved by minimizing the maximum discrepancy of predictors. In this paper, we take this principle further by proposing a more systematic and effective way to achieve hypothesis consistency using Gaussian processes (GP). The GP allows us to induce a hypothesis space of classiﬁers from the posterior distribution of the latent random functions, turning the learning into a largemargin posterior separation problem, signiﬁcantly easier to solve than previous approaches based on adversarial minimax optimization. We formulate a learning objective that effectively inﬂuences the posterior to minimize the maximum discrepancy. This is shown to be equivalent to maximizing margins and minimizing uncertainty of the class predictions in the target domainEmpirical results demonstrate that our approach leads to state-to-the-art performance superior to existing methods on several challenging benchmarks for domain adaptation.},
	language = {en},
	urldate = {2022-10-05},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kim, Minyoung and Sahu, Pritish and Gholami, Behnam and Pavlovic, Vladimir},
	month = jun,
	year = {2019},
	keywords = {notion},
	pages = {4375--4385},
	file = {Kim et al. - 2019 - Unsupervised Visual Domain Adaptation A Deep Max-.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\JNFDUNH7\\Kim et al. - 2019 - Unsupervised Visual Domain Adaptation A Deep Max-.pdf:application/pdf},
}

@misc{chidlovskii_universal_2021,
	title = {Universal {Domain} {Adaptation} in {Ordinal} {Regression}},
	url = {http://arxiv.org/abs/2106.11576},
	doi = {10.48550/arXiv.2106.11576},
	abstract = {We address the problem of universal domain adaptation (UDA) in ordinal regression (OR), which attempts to solve classification problems in which labels are not independent, but follow a natural order. We show that the UDA techniques developed for classification and based on the clustering assumption, under-perform in OR settings. We propose a method that complements the OR classifier with an auxiliary task of order learning, which plays the double role of discriminating between common and private instances, and expanding class labels to the private target images via ranking. Combined with adversarial domain discrimination, our model is able to address the closed set, partial and open set configurations. We evaluate our method on three face age estimation datasets, and show that it outperforms the baseline methods.},
	urldate = {2022-10-07},
	publisher = {arXiv},
	author = {Chidlovskii, Boris and Sadek, Assem and Wolf, Christian},
	month = aug,
	year = {2021},
	note = {arXiv:2106.11576 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\bahi_\\Zotero\\storage\\W3UKIQ3P\\Chidlovskii et al. - 2021 - Universal Domain Adaptation in Ordinal Regression.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bahi_\\Zotero\\storage\\TJ2MPLW4\\2106.html:text/html},
}

@article{wang_domain_2022,
	title = {A {Domain} {Adaptive} {Deep} {Transfer} {Learning} {Method} for {Gas}-{Insulated} {Switchgear} {Partial} {Discharge} {Diagnosis}},
	volume = {37},
	issn = {1937-4208},
	doi = {10.1109/TPWRD.2021.3111862},
	abstract = {Intelligent fault diagnosis methods, especially convolutional neural network (CNN), have made significant progress in gas-insulated switchgear (GIS) partial discharge (PD) diagnosis, which are attributable to two reasons: 1) the training and testing samples come from the identical distribution; 2) there are massive labeled data with PD information. However, owing to the specific operating conditions of GIS, collecting massive samples from the same distribution is difficult in field conditions. With the purpose of resolving the data dilemma of conventional diagnosis methods in the field, we propose a domain adaptive deep transfer learning (DADTL) CNN for small samples GIS PD diagnosis. First, we adopted a CNN to automatically extract transferable features from PD samples. Then, a multilayer DADTL is developed to reduce the marginal and conditional distribution of learned transferable features, and Sliced Wasserstein distance (SWD) is employed as a penalty to reduce the negative migration. The experimental verification was performed on experimental and on-site GIS PD datasets. The results show that DADTL CNN can effectively achieve a high-accuracy diagnosis of GIS PD for small samples. Compared with other methods, the DADTL CNN can achieve more accurate and robust GIS PD diagnosis for on-site small samples.},
	number = {4},
	journal = {IEEE Transactions on Power Delivery},
	author = {Wang, Yanxin and Yan, Jing and Yang, Zhou and Jing, Qianzhen and Qi, Zhengkang and Wang, Jianhua and Geng, Yingsan},
	month = aug,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Power Delivery},
	keywords = {convolutional neural network, Convolutional neural networks, fault diagnosis, Fault diagnosis, Feature extraction, notion, partial discharge, Training},
	pages = {2514--2523},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\5S3K3AEB\\Wang et al. - 2022 - A Domain Adaptive Deep Transfer Learning Method fo.pdf:application/pdf},
}

@article{courty_optimal_2017,
	title = {Optimal {Transport} for {Domain} {Adaptation}},
	volume = {39},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/7586038/},
	doi = {10.1109/TPAMI.2016.2615921},
	abstract = {Domain adaptation is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a speciﬁc data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, ﬁnding domain-invariant representations has shown excellent properties, in particular since it allows to train a unique classiﬁer effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi-supervised case where few labeled samples are available in the target domain.},
	language = {en},
	number = {9},
	urldate = {2022-10-10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Courty, Nicolas and Flamary, Remi and Tuia, Devis and Rakotomamonjy, Alain},
	month = sep,
	year = {2017},
	keywords = {notion},
	pages = {1853--1865},
	file = {Courty et al. - 2017 - Optimal Transport for Domain Adaptation.pdf:C\:\\Users\\bahi_\\Zotero\\storage\\JXQNMV76\\Courty et al. - 2017 - Optimal Transport for Domain Adaptation.pdf:application/pdf},
}

@incollection{csurka_comprehensive_2017,
	address = {Cham},
	series = {Advances in {Computer} {Vision} and {Pattern} {Recognition}},
	title = {A {Comprehensive} {Survey} on {Domain} {Adaptation} for {Visual} {Applications}},
	isbn = {978-3-319-58347-1},
	url = {https://doi.org/10.1007/978-3-319-58347-1_1},
	abstract = {The aim of this chapter is to give an overview of domain adaptation and transfer learning with a specific view to visual applications. After a general motivation, we first position domain adaptation in the more general transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and heterogeneous domain adaptationDomain Adaptation (DA)heterogeneous domain adaptation, heterogeneous DA, (HDA)methods. Third, we discuss the effect of the success of deep convolutional architectures which led to the new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we review DA methods that go beyond image categorization, such as object detection, image segmentation, video analyses or learning visual attributes. We conclude the chapter with a section where we relate domain adaptation to other machine learning solutions.},
	language = {en},
	urldate = {2022-10-11},
	booktitle = {Domain {Adaptation} in {Computer} {Vision} {Applications}},
	publisher = {Springer International Publishing},
	author = {Csurka, Gabriela},
	editor = {Csurka, Gabriela},
	year = {2017},
	doi = {10.1007/978-3-319-58347-1_1},
	keywords = {notion},
	pages = {1--35},
	file = {Full Text PDF:C\:\\Users\\bahi_\\Zotero\\storage\\WFRKD58A\\Csurka - 2017 - A Comprehensive Survey on Domain Adaptation for Vi.pdf:application/pdf},
}
